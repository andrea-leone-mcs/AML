{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Challenge 1"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-04-22T21:22:22.133681Z","iopub.status.busy":"2024-04-22T21:22:22.132825Z","iopub.status.idle":"2024-04-22T21:22:22.140569Z","shell.execute_reply":"2024-04-22T21:22:22.139452Z","shell.execute_reply.started":"2024-04-22T21:22:22.133647Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running locally\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","def is_running_on_kaggle():\n","    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\"\n","DATA_PATH = '/kaggle/input/aerial-cactus/' if is_running_on_kaggle() else 'data/'\n","print('Running on Kaggle' if is_running_on_kaggle() else 'Running locally')\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["Import required libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.142937Z","iopub.status.busy":"2024-04-22T21:22:22.142631Z","iopub.status.idle":"2024-04-22T21:22:22.154116Z","shell.execute_reply":"2024-04-22T21:22:22.153089Z","shell.execute_reply.started":"2024-04-22T21:22:22.142913Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.data import Dataset\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.155358Z","iopub.status.busy":"2024-04-22T21:22:22.155093Z","iopub.status.idle":"2024-04-22T21:22:22.163940Z","shell.execute_reply":"2024-04-22T21:22:22.163168Z","shell.execute_reply.started":"2024-04-22T21:22:22.155320Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":["# Check if CUDA is available\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using {device} device')"]},{"cell_type":"markdown","metadata":{},"source":["Define some useful constants."]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.165555Z","iopub.status.busy":"2024-04-22T21:22:22.165150Z","iopub.status.idle":"2024-04-22T21:22:22.173718Z","shell.execute_reply":"2024-04-22T21:22:22.172943Z","shell.execute_reply.started":"2024-04-22T21:22:22.165525Z"},"trusted":true},"outputs":[],"source":["ANNOTATIONS_FILE = DATA_PATH + 'train.csv'\n","IMG_DIR = DATA_PATH + 'train/train/'\n","CHECKPOINT_DIR = '/kaggle/working/checkpoints' if is_running_on_kaggle() else 'checkpoints/'\n","FIGURES_DIR = '/kaggle/working/figures' if is_running_on_kaggle() else 'figures/'\n","\n","SEED = 42\n","BATCH_SIZE = 64\n","LEARNING_RATE = 1e-3\n","TRAIN_SPLIT = 0.8\n","EPOCHS = 40 if is_running_on_kaggle() else 5"]},{"cell_type":"markdown","metadata":{},"source":["Set the manual seed."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.176318Z","iopub.status.busy":"2024-04-22T21:22:22.175754Z","iopub.status.idle":"2024-04-22T21:22:22.185512Z","shell.execute_reply":"2024-04-22T21:22:22.184762Z","shell.execute_reply.started":"2024-04-22T21:22:22.176293Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x725ca3047090>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["Extend **Dataset** class for the **DatasetLoader** (define a mapping for images and labels)."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.186858Z","iopub.status.busy":"2024-04-22T21:22:22.186595Z","iopub.status.idle":"2024-04-22T21:22:22.194881Z","shell.execute_reply":"2024-04-22T21:22:22.194111Z","shell.execute_reply.started":"2024-04-22T21:22:22.186836Z"},"trusted":true},"outputs":[],"source":["from torchvision.io import read_image\n","\n","class CactusDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"markdown","metadata":{},"source":["TODO: Try to preprocess like in ImProc"]},{"cell_type":"markdown","metadata":{},"source":["Instanciate a **Dataset** object on the training (+validation) data."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.196146Z","iopub.status.busy":"2024-04-22T21:22:22.195897Z","iopub.status.idle":"2024-04-22T21:22:22.207541Z","shell.execute_reply":"2024-04-22T21:22:22.206742Z","shell.execute_reply.started":"2024-04-22T21:22:22.196124Z"},"trusted":true},"outputs":[],"source":["# Imagenet mean and std\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.268268Z","iopub.status.busy":"2024-04-22T21:22:22.267674Z","iopub.status.idle":"2024-04-22T21:22:22.273158Z","shell.execute_reply":"2024-04-22T21:22:22.272279Z","shell.execute_reply.started":"2024-04-22T21:22:22.268239Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as transforms\n","\n","# Transformation for the image data\n","transform = transforms.Compose([\n","    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n","    transforms.CenterCrop(224),\n","    transforms.ConvertImageDtype(torch.float32),\n","    transforms.Normalize(mean=mean, std=std),\n","])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.275790Z","iopub.status.busy":"2024-04-22T21:22:22.275103Z","iopub.status.idle":"2024-04-22T21:22:22.305046Z","shell.execute_reply":"2024-04-22T21:22:22.304320Z","shell.execute_reply.started":"2024-04-22T21:22:22.275759Z"},"trusted":true},"outputs":[],"source":["# Create the dataset object\n","trainval_data = CactusDataset(ANNOTATIONS_FILE, IMG_DIR, transform=transform)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.306270Z","iopub.status.busy":"2024-04-22T21:22:22.306014Z","iopub.status.idle":"2024-04-22T21:22:22.316179Z","shell.execute_reply":"2024-04-22T21:22:22.315183Z","shell.execute_reply.started":"2024-04-22T21:22:22.306248Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 224, 224])\n"]}],"source":["# Print the shape of the first image in the dataset\n","print(trainval_data[0][0].shape)"]},{"cell_type":"markdown","metadata":{},"source":["Split the dataset into train + validation"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.319055Z","iopub.status.busy":"2024-04-22T21:22:22.318770Z","iopub.status.idle":"2024-04-22T21:22:22.327259Z","shell.execute_reply":"2024-04-22T21:22:22.326323Z","shell.execute_reply.started":"2024-04-22T21:22:22.319031Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 14000\n","Validation set size: 3500\n"]}],"source":["from torch.utils.data import random_split\n","\n","# Define the sizes of training and validation sets\n","train_size = int(TRAIN_SPLIT * len(trainval_data))\n","val_size = len(trainval_data) - train_size\n","\n","# Split the dataset into training and validation sets\n","train_data, val_data = random_split(trainval_data, [train_size, val_size])\n","\n","# Print the sizes of the training and validation sets\n","print(\"Training set size:\", len(train_data))\n","print(\"Validation set size:\", len(val_data))"]},{"cell_type":"markdown","metadata":{},"source":["Let's define our first model.\\\n","We are going to use the ResNet18 pretrained model and then we are going to add 1 linear FC output layer. The output will be a real value that we will feed into a Sigmoid function to squash it into the $[0, 1]$ interval, and we will do the classification by comparing the output of the Sigmoid with the $0.5$ treshold.\\\n","Since we don't want to adjust ResNet18 weights, we are going to set the **requires_grad** property to **False** for each of its parameters."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.328449Z","iopub.status.busy":"2024-04-22T21:22:22.328169Z","iopub.status.idle":"2024-04-22T21:22:22.337077Z","shell.execute_reply":"2024-04-22T21:22:22.336277Z","shell.execute_reply.started":"2024-04-22T21:22:22.328427Z"},"trusted":true},"outputs":[],"source":["from torchvision.models import vgg16, vgg16_weights\n","\n","class VGGClassifier(nn.Module):\n","    def __init__(self, fc_size=None):\n","        super(VGGClassifier, self).__init__()\n","        \n","        vgg = vgg16(weights=vgg16_weights.IMAGENET1K_V1)\n","        for param in vgg.parameters():\n","            param.requires_grad = False\n","        \n","        self.vgg = vgg\n","        self.fc1 = nn.Linear(vgg.fc.out_features, fc_size)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.3)\n","        self.fc2 = nn.Linear(fc_size, 1)\n","\n","    def forward(self, x):\n","        x = self.vgg(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["At this point, let's train our model."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.338726Z","iopub.status.busy":"2024-04-22T21:22:22.338213Z","iopub.status.idle":"2024-04-22T21:22:22.351556Z","shell.execute_reply":"2024-04-22T21:22:22.350772Z","shell.execute_reply.started":"2024-04-22T21:22:22.338699Z"},"trusted":true},"outputs":[],"source":["# Utility function for saving epochs checkpoints\n","def save_checkpoint(model, optimizer, loss, dir, desc):\n","    checkpoint_dir = os.path.join(CHECKPOINT_DIR, dir)\n","    if not os.path.exists(checkpoint_dir):\n","        os.makedirs(checkpoint_dir)\n","    \n","    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_{desc}.pt')\n","    torch.save({\n","        'desc': desc,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss,\n","    }, checkpoint_path)\n","\n","# Utility function for loading epochs checkpoints\n","def load_checkpoint(model, optimizer, dir, desc):\n","    checkpoint_path = os.path.join(CHECKPOINT_DIR, dir, f'checkpoint_{desc}.pt')\n","    checkpoint = torch.load(checkpoint_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    loss = checkpoint['loss']\n","    return model, optimizer, loss"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.352887Z","iopub.status.busy":"2024-04-22T21:22:22.352631Z","iopub.status.idle":"2024-04-22T21:22:22.363046Z","shell.execute_reply":"2024-04-22T21:22:22.362269Z","shell.execute_reply.started":"2024-04-22T21:22:22.352866Z"},"trusted":true},"outputs":[],"source":["# Training loop (1 epoch)\n","def train_model(train_dataloader, model, criterion, optimizer, checkpoint=False, desc='Training', dir_checkpoint=None):\n","    avg_train_loss = 0\n","    train_bar = tqdm(train_dataloader, desc=desc, leave=False)\n","    \n","    model.train()\n","    for X, y in train_bar:\n","        X = X.to(device)\n","        y = y.to(device)\n","\n","        # Compute prediction and loss\n","        pred = model(X.float())\n","        y = y.reshape(-1, 1).float()\n","        loss = criterion(pred, y)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the average loss\n","        avg_train_loss += loss.item() * len(y)\n","\n","        # Update the loading bar\n","        train_bar.set_postfix({'loss': loss.item()})\n","        \n","    avg_train_loss = avg_train_loss / len(train_dataloader.dataset)\n","    train_bar.set_postfix({'loss': avg_train_loss})\n","    train_bar.close()\n","\n","    if checkpoint:\n","        save_checkpoint(model, optimizer, avg_train_loss, dir_checkpoint, desc)\n","\n","    return avg_train_loss"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.364273Z","iopub.status.busy":"2024-04-22T21:22:22.364010Z","iopub.status.idle":"2024-04-22T21:22:22.376834Z","shell.execute_reply":"2024-04-22T21:22:22.375855Z","shell.execute_reply.started":"2024-04-22T21:22:22.364251Z"},"trusted":true},"outputs":[],"source":["# Compute validation loss and error rate\n","def evaluate(val_dataloader, model, criterion, desc='Validation'):\n","    avg_val_loss = 0\n","    errors = []\n","    val_bar = tqdm(val_dataloader, desc=desc, leave=False)\n","\n","    model.eval()\n","    tp = 0\n","    tn = 0\n","    fp = 0\n","    fn = 0\n","\n","    with torch.no_grad():\n","        for X, y in val_bar:\n","            X = X.to(device)\n","            y = y.to(device)\n","\n","            # Compute prediction and loss\n","            pred = model(X.float())\n","            y = y.reshape(-1, 1).float()\n","            loss = criterion(pred, y)\n","\n","            # Save errors for error rate\n","            pred = nn.Sigmoid()(pred) > 0.5\n","\n","            tp += torch.bitwise_and(y == 1, pred == 1).sum().item()\n","            tn += torch.bitwise_and(y == 0, pred == 0).sum().item()\n","            fp += torch.bitwise_and(y == 0, pred == 1).sum().item()\n","            fn += torch.bitwise_and(y == 1, pred == 0).sum().item()\n","\n","            errors += pred != y\n","\n","            # Update the average loss\n","            avg_val_loss += loss.item() * len(y)\n","\n","            # Update the loading bar\n","            val_bar.set_postfix({'loss': loss.item()})\n","    \n","    avg_val_loss = avg_val_loss / len(val_dataloader.dataset)\n","    val_bar.set_postfix({'loss': avg_val_loss})\n","    val_bar.close()\n","    return avg_val_loss, errors, [[tp, fp], [fn, tn]]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# Early stopping\n","class EarlyStopper:\n","    def __init__(self, patience=1, min_delta=0):\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.min_validation_loss = float('inf')\n","\n","    def early_stop(self, validation_loss):\n","        if validation_loss < self.min_validation_loss:\n","            self.min_validation_loss = validation_loss\n","            self.counter = 0\n","        elif validation_loss > (self.min_validation_loss + self.min_delta):\n","            self.counter += 1\n","            if self.counter >= self.patience:\n","                return True\n","        return False\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def f1_score(confusion_matrix):\n","    tp, fp = confusion_matrix[0]\n","    fn, _ = confusion_matrix[1]\n","    precision = tp / (tp + fp)\n","    recall = tp / (tp + fn)\n","    return 2 * (precision * recall) / (precision + recall)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-04-22T21:22:22.378266Z","iopub.status.busy":"2024-04-22T21:22:22.377904Z","iopub.status.idle":"2024-04-22T22:39:46.471114Z","shell.execute_reply":"2024-04-22T22:39:46.470127Z","shell.execute_reply.started":"2024-04-22T21:22:22.378240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running Training for 5 epochs\n",">>>>>>>>     FC Size: 1     <<<<<<<<\n"]},{"name":"stderr","output_type":"stream","text":["                                                                            \r"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrainingEpoch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m02d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     val_loss, errors, confusion_matrix \u001b[38;5;241m=\u001b[39m evaluate(val_dataloader, model, criterion, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidationEpoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, model, criterion, optimizer, checkpoint, desc, dir_checkpoint)\u001b[0m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m----> 8\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["fc_sizes = [1, 6, 12, 24]\n","\n","best_models = []\n","\n","print(f'Running Training for {EPOCHS} epochs')\n","\n","for fc_size in fc_sizes:\n","    print(f'>>>>>>>>     FC Size: {fc_size}     <<<<<<<<')\n","    # Create the model\n","    model = VGGClassifier(fc_size=fc_size).to(device)\n","\n","    # Define the loss function and optimizer\n","    criterion = nn.BCEWithLogitsLoss().to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    # Create the dataloaders\n","    train_dataloader = torch.utils.data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n","    val_dataloader = torch.utils.data.DataLoader(val_data, BATCH_SIZE, shuffle=False)\n","\n","    train_losses = []\n","    val_losses = []\n","    err_rates = []\n","    f1_scores = []\n","    early_stopper = EarlyStopper(patience=10, min_delta=5e-4)\n","    best_epoch = None\n","\n","    for epoch in range(EPOCHS):\n","        train_loss = train_model(train_dataloader, model, criterion, optimizer, checkpoint=False, desc=f'TrainingEpoch{(epoch + 1):02d}')\n","        val_loss, errors, confusion_matrix = evaluate(val_dataloader, model, criterion, desc=f'ValidationEpoch{(epoch + 1):02d}')\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        error_rate = (sum(errors) / len(val_data)).item()\n","        err_rates.append(error_rate)\n","        f1 = f1_score(confusion_matrix)\n","        f1_scores.append(f1)\n","        fpr = confusion_matrix[0][1] / (confusion_matrix[0][0] + confusion_matrix[0][1])\n","        fnr = confusion_matrix[1][0] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n","        print(f'Epoch {epoch + 1}/{EPOCHS}\\t Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Error Rate: {error_rate:.4f} - F1 Score: {f1:.4f} - (FPR: {fpr:.4f} - FNR: {fnr:.4f})')\n","        if best_epoch is None or val_loss < val_losses[best_epoch]:\n","            best_epoch = epoch\n","            save_checkpoint(model, optimizer, val_loss, f'resnet/fc_{fc_size}', f'epoch{epoch+1}')\n","\n","        if early_stopper.early_stop(val_loss):\n","            print(f'Early stopping on epoch {epoch + 1}')\n","            break\n","    \n","    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n","    fig.suptitle(f'FC Size: {fc_size}')\n","    \n","    ax[0].set_title('Loss')\n","    ax[1].set_title('Error Rate (validation)')\n","    \n","    epochs = [i+1 for i in range(len(train_losses))]\n","    ax[0].plot(epochs, train_losses, label='Train Loss', color='tab:blue')\n","    ax[0].plot(epochs, val_losses, label='Val Loss', color='tab:orange')\n","    ax[1].plot(epochs, err_rates, label='Error Rate', color='tab:blue')\n","    ax2 = ax[1].twinx()\n","    ax2.plot(epochs, f1_scores, color='tab:orange', label='F1 Score')\n","    \n","    epochs = [i for i in range(0, len(train_losses)+1, 5)]\n","    ax[0].set_xlabel('Epoch')\n","    ax[0].set_xticks(epochs)\n","    ax[0].set_ylabel('Loss')\n","    ax[0].legend()\n","    ax[1].set_xlabel('Epoch')\n","    ax[1].set_xticks(epochs)\n","    ax[1].set_ylabel('Error Rate')\n","    ax[1].legend()\n","    ax2.set_ylabel('F1 Score')\n","    ax2.tick_params(axis='y', labelcolor='tab:orange')\n","    # Add legend\n","    lines, labels = ax[1].get_legend_handles_labels()\n","    lines2, labels2 = ax2.get_legend_handles_labels()\n","    ax2.legend(lines + lines2, labels + labels2, loc='best')\n","\n","    plt.show()\n","    fig_dir = os.path.join(FIGURES_DIR, f'resnet')\n","    if not os.path.exists(fig_dir):\n","        os.makedirs(fig_dir)\n","    fig_path = os.path.join(fig_dir, f'fc_{fc_size}.png')\n","    \n","    fig.savefig(fig_path)\n","\n","    best_models.append({\n","        'fc_size': fc_size,\n","        'best_epoch': best_epoch,\n","        'train_loss': train_losses[best_epoch],\n","        'val_loss': val_losses[best_epoch],\n","        'error_rate': err_rates[best_epoch],\n","        'f1_score': f1_scores[best_epoch],\n","        'fig_path': fig_path,\n","    })"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = ResnetClassifier(fc_size=1).to(device)\n","# criterion = nn.BCEWithLogitsLoss().to(device)\n","# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","# model = load_checkpoint(model, optimizer, 'fc_size_1', 'TrainingEpoch05')[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 224, 224])\n"]},{"name":"stderr","output_type":"stream","text":["Testing:   0%|          | 0/63 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["                                                        \r"]}],"source":["# # Define the path to the test annotations file and test image directory\n","# TEST_ANNO_FILE = DATA_PATH + 'sample_submission.csv'\n","# TEST_IMG_DIR = DATA_PATH + 'test/test/'\n","\n","# # Create the test dataset object\n","# test_data = CactusDataset(TEST_ANNO_FILE, TEST_IMG_DIR, transform=transform)\n","\n","# # Print the shape of the first image in the test dataset\n","# print(test_data[0][0].shape)\n","\n","# # Create the test dataloader\n","# test_dataloader = torch.utils.data.DataLoader(test_data, BATCH_SIZE, shuffle=False)\n","\n","# # Compute the predictions\n","# predictions = []\n","# model.eval()\n","# with torch.no_grad():\n","#     for X, _ in tqdm(test_dataloader, desc='Testing', leave=False):\n","#         X = X.to(device)\n","#         pred = model(X.float())\n","#         pred = nn.Sigmoid()(pred) > 0.5\n","#         predictions += pred.cpu().numpy().flatten().tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# predictions = torch.tensor(predictions).int()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4000])\n","tensor(2999)\n","tensor(1001)\n"]}],"source":["# print(predictions.shape)\n","# print(predictions.sum())\n","# tmp = torch.tensor(1) - predictions\n","# print(tmp.sum())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           9,408\n","       BatchNorm2d-2           [-1, 64, 16, 16]             128\n","              ReLU-3           [-1, 64, 16, 16]               0\n","         MaxPool2d-4             [-1, 64, 8, 8]               0\n","            Conv2d-5             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-6             [-1, 64, 8, 8]             128\n","              ReLU-7             [-1, 64, 8, 8]               0\n","            Conv2d-8             [-1, 64, 8, 8]          36,864\n","       BatchNorm2d-9             [-1, 64, 8, 8]             128\n","             ReLU-10             [-1, 64, 8, 8]               0\n","       BasicBlock-11             [-1, 64, 8, 8]               0\n","           Conv2d-12             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-13             [-1, 64, 8, 8]             128\n","             ReLU-14             [-1, 64, 8, 8]               0\n","           Conv2d-15             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-16             [-1, 64, 8, 8]             128\n","             ReLU-17             [-1, 64, 8, 8]               0\n","       BasicBlock-18             [-1, 64, 8, 8]               0\n","           Conv2d-19            [-1, 128, 4, 4]          73,728\n","      BatchNorm2d-20            [-1, 128, 4, 4]             256\n","             ReLU-21            [-1, 128, 4, 4]               0\n","           Conv2d-22            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-23            [-1, 128, 4, 4]             256\n","           Conv2d-24            [-1, 128, 4, 4]           8,192\n","      BatchNorm2d-25            [-1, 128, 4, 4]             256\n","             ReLU-26            [-1, 128, 4, 4]               0\n","       BasicBlock-27            [-1, 128, 4, 4]               0\n","           Conv2d-28            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-29            [-1, 128, 4, 4]             256\n","             ReLU-30            [-1, 128, 4, 4]               0\n","           Conv2d-31            [-1, 128, 4, 4]         147,456\n","      BatchNorm2d-32            [-1, 128, 4, 4]             256\n","             ReLU-33            [-1, 128, 4, 4]               0\n","       BasicBlock-34            [-1, 128, 4, 4]               0\n","           Conv2d-35            [-1, 256, 2, 2]         294,912\n","      BatchNorm2d-36            [-1, 256, 2, 2]             512\n","             ReLU-37            [-1, 256, 2, 2]               0\n","           Conv2d-38            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-39            [-1, 256, 2, 2]             512\n","           Conv2d-40            [-1, 256, 2, 2]          32,768\n","      BatchNorm2d-41            [-1, 256, 2, 2]             512\n","             ReLU-42            [-1, 256, 2, 2]               0\n","       BasicBlock-43            [-1, 256, 2, 2]               0\n","           Conv2d-44            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-45            [-1, 256, 2, 2]             512\n","             ReLU-46            [-1, 256, 2, 2]               0\n","           Conv2d-47            [-1, 256, 2, 2]         589,824\n","      BatchNorm2d-48            [-1, 256, 2, 2]             512\n","             ReLU-49            [-1, 256, 2, 2]               0\n","       BasicBlock-50            [-1, 256, 2, 2]               0\n","           Conv2d-51            [-1, 512, 1, 1]       1,179,648\n","      BatchNorm2d-52            [-1, 512, 1, 1]           1,024\n","             ReLU-53            [-1, 512, 1, 1]               0\n","           Conv2d-54            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-55            [-1, 512, 1, 1]           1,024\n","           Conv2d-56            [-1, 512, 1, 1]         131,072\n","      BatchNorm2d-57            [-1, 512, 1, 1]           1,024\n","             ReLU-58            [-1, 512, 1, 1]               0\n","       BasicBlock-59            [-1, 512, 1, 1]               0\n","           Conv2d-60            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-61            [-1, 512, 1, 1]           1,024\n","             ReLU-62            [-1, 512, 1, 1]               0\n","           Conv2d-63            [-1, 512, 1, 1]       2,359,296\n","      BatchNorm2d-64            [-1, 512, 1, 1]           1,024\n","             ReLU-65            [-1, 512, 1, 1]               0\n","       BasicBlock-66            [-1, 512, 1, 1]               0\n","AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n","           Linear-68                 [-1, 1000]         513,000\n","           ResNet-69                 [-1, 1000]               0\n","           Linear-70                    [-1, 1]           1,001\n","================================================================\n","Total params: 11,690,513\n","Trainable params: 1,001\n","Non-trainable params: 11,689,512\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.30\n","Params size (MB): 44.60\n","Estimated Total Size (MB): 45.91\n","----------------------------------------------------------------\n"]}],"source":["# from torchsummary import summary\n","\n","# summary(model, (3, 32, 32))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4736944,"sourceId":8035608,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
