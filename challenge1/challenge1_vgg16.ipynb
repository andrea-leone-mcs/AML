{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8035608,"sourceType":"datasetVersion","datasetId":4736944}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\ndef is_running_on_kaggle():\n    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\"\nDATA_PATH = '/kaggle/input/aerial-cactus/' if is_running_on_kaggle() else 'data/'\nprint('Running on Kaggle' if is_running_on_kaggle() else 'Running locally')\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-29T15:58:25.170091Z","iopub.execute_input":"2024-04-29T15:58:25.170941Z","iopub.status.idle":"2024-04-29T15:58:25.179013Z","shell.execute_reply.started":"2024-04-29T15:58:25.170894Z","shell.execute_reply":"2024-04-29T15:58:25.177667Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Running on Kaggle\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torch.utils.data import Dataset\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.181417Z","iopub.execute_input":"2024-04-29T15:58:25.181861Z","iopub.status.idle":"2024-04-29T15:58:25.191904Z","shell.execute_reply.started":"2024-04-29T15:58:25.181818Z","shell.execute_reply":"2024-04-29T15:58:25.190989Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Check if CUDA is available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using {device} device')","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.193349Z","iopub.execute_input":"2024-04-29T15:58:25.193735Z","iopub.status.idle":"2024-04-29T15:58:25.256798Z","shell.execute_reply.started":"2024-04-29T15:58:25.193687Z","shell.execute_reply":"2024-04-29T15:58:25.255878Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}]},{"cell_type":"code","source":"ANNOTATIONS_FILE = DATA_PATH + 'train.csv'\nIMG_DIR = DATA_PATH + 'train/train/'\nCHECKPOINT_DIR = '/kaggle/working/checkpoints' if is_running_on_kaggle() else 'checkpoints/'\nFIGURES_DIR = '/kaggle/working/figures' if is_running_on_kaggle() else 'figures/'\n\nSEED = 42\nBATCH_SIZE = 64\nLEARNING_RATE = 1e-3\nTRAIN_SPLIT = 0.8\nEPOCHS = 40 if is_running_on_kaggle() else 5","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.257957Z","iopub.execute_input":"2024-04-29T15:58:25.258652Z","iopub.status.idle":"2024-04-29T15:58:25.264217Z","shell.execute_reply.started":"2024-04-29T15:58:25.258625Z","shell.execute_reply":"2024-04-29T15:58:25.263241Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.266380Z","iopub.execute_input":"2024-04-29T15:58:25.266678Z","iopub.status.idle":"2024-04-29T15:58:25.277641Z","shell.execute_reply.started":"2024-04-29T15:58:25.266655Z","shell.execute_reply":"2024-04-29T15:58:25.276660Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7be4c0c4ac90>"},"metadata":{}}]},{"cell_type":"code","source":"from torchvision.io import read_image\n\nclass CactusDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n        image = read_image(img_path)\n        label = self.img_labels.iloc[idx, 1]\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.278937Z","iopub.execute_input":"2024-04-29T15:58:25.279617Z","iopub.status.idle":"2024-04-29T15:58:25.287201Z","shell.execute_reply.started":"2024-04-29T15:58:25.279592Z","shell.execute_reply":"2024-04-29T15:58:25.286084Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Imagenet mean and std\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.289566Z","iopub.execute_input":"2024-04-29T15:58:25.291056Z","iopub.status.idle":"2024-04-29T15:58:25.297903Z","shell.execute_reply.started":"2024-04-29T15:58:25.291024Z","shell.execute_reply":"2024-04-29T15:58:25.296999Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import torchvision.transforms as transforms\n\n# Transformation for the image data\ntransform = transforms.Compose([\n    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n    transforms.CenterCrop(224),\n    transforms.ConvertImageDtype(torch.float32),\n    transforms.Normalize(mean=mean, std=std),\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.300064Z","iopub.execute_input":"2024-04-29T15:58:25.300414Z","iopub.status.idle":"2024-04-29T15:58:25.308521Z","shell.execute_reply.started":"2024-04-29T15:58:25.300366Z","shell.execute_reply":"2024-04-29T15:58:25.307500Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create the dataset object\ntrainval_data = CactusDataset(ANNOTATIONS_FILE, IMG_DIR, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.309576Z","iopub.execute_input":"2024-04-29T15:58:25.309898Z","iopub.status.idle":"2024-04-29T15:58:25.378398Z","shell.execute_reply.started":"2024-04-29T15:58:25.309874Z","shell.execute_reply":"2024-04-29T15:58:25.377417Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Print the shape of the first image in the dataset\nprint(trainval_data[0][0].shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.379759Z","iopub.execute_input":"2024-04-29T15:58:25.380122Z","iopub.status.idle":"2024-04-29T15:58:25.477421Z","shell.execute_reply.started":"2024-04-29T15:58:25.380088Z","shell.execute_reply":"2024-04-29T15:58:25.476517Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"torch.Size([3, 224, 224])\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\n# Define the sizes of training and validation sets\ntrain_size = int(TRAIN_SPLIT * len(trainval_data))\nval_size = len(trainval_data) - train_size\n\n# Split the dataset into training and validation sets\ntrain_data, val_data = random_split(trainval_data, [train_size, val_size])\n\n# Print the sizes of the training and validation sets\nprint(\"Training set size:\", len(train_data))\nprint(\"Validation set size:\", len(val_data))","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.481186Z","iopub.execute_input":"2024-04-29T15:58:25.481796Z","iopub.status.idle":"2024-04-29T15:58:25.492854Z","shell.execute_reply.started":"2024-04-29T15:58:25.481761Z","shell.execute_reply":"2024-04-29T15:58:25.491670Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training set size: 14000\nValidation set size: 3500\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.models import vgg16, VGG16_Weights\n\nclass VGGClassifier(nn.Module):\n    def __init__(self, fc_size=None):\n        super(VGGClassifier, self).__init__()\n        \n        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n        for param in vgg.parameters():\n            param.requires_grad = False\n        \n        self.vgg = vgg\n        self.fc1 = nn.Linear(vgg.classifier[-1].out_features, fc_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(fc_size, 1)\n\n    def forward(self, x):\n        x = self.vgg(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.494072Z","iopub.execute_input":"2024-04-29T15:58:25.494695Z","iopub.status.idle":"2024-04-29T15:58:25.502208Z","shell.execute_reply.started":"2024-04-29T15:58:25.494670Z","shell.execute_reply":"2024-04-29T15:58:25.501234Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Utility function for saving epochs checkpoints\ndef save_checkpoint(model, optimizer, loss, dir, desc):\n    checkpoint_dir = os.path.join(CHECKPOINT_DIR, dir)\n    if not os.path.exists(checkpoint_dir):\n        os.makedirs(checkpoint_dir)\n    \n    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_{desc}.pt')\n    torch.save({\n        'desc': desc,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'loss': loss,\n    }, checkpoint_path)\n\n# Utility function for loading epochs checkpoints\ndef load_checkpoint(model, optimizer, dir, desc):\n    checkpoint_path = os.path.join(CHECKPOINT_DIR, dir, f'checkpoint_{desc}.pt')\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    loss = checkpoint['loss']\n    return model, optimizer, loss","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.503344Z","iopub.execute_input":"2024-04-29T15:58:25.503690Z","iopub.status.idle":"2024-04-29T15:58:25.514314Z","shell.execute_reply.started":"2024-04-29T15:58:25.503660Z","shell.execute_reply":"2024-04-29T15:58:25.513478Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Training loop (1 epoch)\ndef train_model(train_dataloader, model, criterion, optimizer, checkpoint=False, desc='Training', dir_checkpoint=None):\n    avg_train_loss = 0\n    train_bar = tqdm(train_dataloader, desc=desc, leave=False)\n    \n    model.train()\n    for X, y in train_bar:\n        X = X.to(device)\n        y = y.to(device)\n\n        # Compute prediction and loss\n        pred = model(X.float())\n        y = y.reshape(-1, 1).float()\n        loss = criterion(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Update the average loss\n        avg_train_loss += loss.item() * len(y)\n\n        # Update the loading bar\n        train_bar.set_postfix({'loss': loss.item()})\n        \n    avg_train_loss = avg_train_loss / len(train_dataloader.dataset)\n    train_bar.set_postfix({'loss': avg_train_loss})\n    train_bar.close()\n\n    if checkpoint:\n        save_checkpoint(model, optimizer, avg_train_loss, dir_checkpoint, desc)\n\n    return avg_train_loss","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.515676Z","iopub.execute_input":"2024-04-29T15:58:25.516542Z","iopub.status.idle":"2024-04-29T15:58:25.528007Z","shell.execute_reply.started":"2024-04-29T15:58:25.516512Z","shell.execute_reply":"2024-04-29T15:58:25.527038Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Compute validation loss and error rate\ndef evaluate(val_dataloader, model, criterion, desc='Validation'):\n    avg_val_loss = 0\n    errors = []\n    val_bar = tqdm(val_dataloader, desc=desc, leave=False)\n\n    model.eval()\n    tp = 0\n    tn = 0\n    fp = 0\n    fn = 0\n\n    with torch.no_grad():\n        for X, y in val_bar:\n            X = X.to(device)\n            y = y.to(device)\n\n            # Compute prediction and loss\n            pred = model(X.float())\n            y = y.reshape(-1, 1).float()\n            loss = criterion(pred, y)\n\n            # Save errors for error rate\n            pred = nn.Sigmoid()(pred) > 0.5\n\n            tp += torch.bitwise_and(y == 1, pred == 1).sum().item()\n            tn += torch.bitwise_and(y == 0, pred == 0).sum().item()\n            fp += torch.bitwise_and(y == 0, pred == 1).sum().item()\n            fn += torch.bitwise_and(y == 1, pred == 0).sum().item()\n\n            errors += pred != y\n\n            # Update the average loss\n            avg_val_loss += loss.item() * len(y)\n\n            # Update the loading bar\n            val_bar.set_postfix({'loss': loss.item()})\n    \n    avg_val_loss = avg_val_loss / len(val_dataloader.dataset)\n    val_bar.set_postfix({'loss': avg_val_loss})\n    val_bar.close()\n    return avg_val_loss, errors, [[tp, fp], [fn, tn]]","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.529663Z","iopub.execute_input":"2024-04-29T15:58:25.530104Z","iopub.status.idle":"2024-04-29T15:58:25.545260Z","shell.execute_reply.started":"2024-04-29T15:58:25.530047Z","shell.execute_reply":"2024-04-29T15:58:25.544334Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Early stopping\nclass EarlyStopper:\n    def __init__(self, patience=1, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.min_validation_loss = float('inf')\n\n    def early_stop(self, validation_loss):\n        if validation_loss < self.min_validation_loss:\n            self.min_validation_loss = validation_loss\n            self.counter = 0\n        elif validation_loss > (self.min_validation_loss + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.546640Z","iopub.execute_input":"2024-04-29T15:58:25.547010Z","iopub.status.idle":"2024-04-29T15:58:25.559451Z","shell.execute_reply.started":"2024-04-29T15:58:25.546979Z","shell.execute_reply":"2024-04-29T15:58:25.558511Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def f1_score(confusion_matrix):\n    tp, fp = confusion_matrix[0]\n    fn, _ = confusion_matrix[1]\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    return 2 * (precision * recall) / (precision + recall)","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.561150Z","iopub.execute_input":"2024-04-29T15:58:25.561501Z","iopub.status.idle":"2024-04-29T15:58:25.568597Z","shell.execute_reply.started":"2024-04-29T15:58:25.561469Z","shell.execute_reply":"2024-04-29T15:58:25.567923Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"fc_sizes = [1, 6, 12, 24]\n\nbest_models = []\n\nprint(f'Running Training for {EPOCHS} epochs')\n\nfor fc_size in fc_sizes:\n    print(f'>>>>>>>>     FC Size: {fc_size}     <<<<<<<<')\n    # Create the model\n    model = VGGClassifier(fc_size=fc_size).to(device)\n\n    # Define the loss function and optimizer\n    criterion = nn.BCEWithLogitsLoss().to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    # Create the dataloaders\n    train_dataloader = torch.utils.data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n    val_dataloader = torch.utils.data.DataLoader(val_data, BATCH_SIZE, shuffle=False)\n\n    train_losses = []\n    val_losses = []\n    err_rates = []\n    f1_scores = []\n    early_stopper = EarlyStopper(patience=10, min_delta=5e-4)\n    best_epoch = None\n\n    for epoch in range(EPOCHS):\n        train_loss = train_model(train_dataloader, model, criterion, optimizer, checkpoint=False, desc=f'TrainingEpoch{(epoch + 1):02d}')\n        val_loss, errors, confusion_matrix = evaluate(val_dataloader, model, criterion, desc=f'ValidationEpoch{(epoch + 1):02d}')\n        train_losses.append(train_loss)\n        val_losses.append(val_loss)\n        error_rate = (sum(errors) / len(val_data)).item()\n        err_rates.append(error_rate)\n        f1 = f1_score(confusion_matrix)\n        f1_scores.append(f1)\n        fpr = 0\n        fnr = 0\n        print(confusion_matrix)\n        if (confusion_matrix[0][0] + confusion_matrix[0][1]) != 0:\n            fpr = confusion_matrix[0][1] / (confusion_matrix[0][0] + confusion_matrix[0][1])\n        if (confusion_matrix[1][0] + confusion_matrix[1][1]) != 0:\n            fnr = confusion_matrix[1][0] / (confusion_matrix[1][0] + confusion_matrix[1][1])\n        print(f'Epoch {epoch + 1}/{EPOCHS}\\t Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Error Rate: {error_rate:.4f} - F1 Score: {f1:.4f} - (FPR: {fpr:.4f} - FNR: {fnr:.4f})')\n        if best_epoch is None or val_loss < val_losses[best_epoch]:\n            best_epoch = epoch\n            save_checkpoint(model, optimizer, val_loss, f'resnet/fc_{fc_size}', f'epoch{epoch+1}')\n\n        if early_stopper.early_stop(val_loss):\n            print(f'Early stopping on epoch {epoch + 1}')\n            break\n    \n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    fig.suptitle(f'FC Size: {fc_size}')\n    \n    ax[0].set_title('Loss')\n    ax[1].set_title('Error Rate (validation)')\n    \n    epochs = [i+1 for i in range(len(train_losses))]\n    ax[0].plot(epochs, train_losses, label='Train Loss', color='tab:blue')\n    ax[0].plot(epochs, val_losses, label='Val Loss', color='tab:orange')\n    ax[1].plot(epochs, err_rates, label='Error Rate', color='tab:blue')\n    ax2 = ax[1].twinx()\n    ax2.plot(epochs, f1_scores, color='tab:orange', label='F1 Score')\n    \n    epochs = [i for i in range(0, len(train_losses)+1, 5)]\n    ax[0].set_xlabel('Epoch')\n    ax[0].set_xticks(epochs)\n    ax[0].set_ylabel('Loss')\n    ax[0].legend()\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_xticks(epochs)\n    ax[1].set_ylabel('Error Rate')\n    ax[1].legend()\n    ax2.set_ylabel('F1 Score')\n    ax2.tick_params(axis='y', labelcolor='tab:orange')\n    # Add legend\n    lines, labels = ax[1].get_legend_handles_labels()\n    lines2, labels2 = ax2.get_legend_handles_labels()\n    ax2.legend(lines + lines2, labels + labels2, loc='best')\n\n    plt.show()\n    fig_dir = os.path.join(FIGURES_DIR, f'resnet')\n    if not os.path.exists(fig_dir):\n        os.makedirs(fig_dir)\n    fig_path = os.path.join(fig_dir, f'fc_{fc_size}.png')\n    \n    fig.savefig(fig_path)\n\n    best_models.append({\n        'fc_size': fc_size,\n        'best_epoch': best_epoch,\n        'train_loss': train_losses[best_epoch],\n        'val_loss': val_losses[best_epoch],\n        'error_rate': err_rates[best_epoch],\n        'f1_score': f1_scores[best_epoch],\n        'fig_path': fig_path,\n    })","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:58:25.570055Z","iopub.execute_input":"2024-04-29T15:58:25.570673Z","iopub.status.idle":"2024-04-29T16:06:58.210162Z","shell.execute_reply.started":"2024-04-29T15:58:25.570637Z","shell.execute_reply":"2024-04-29T16:06:58.208728Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Running Training for 40 epochs\n>>>>>>>>     FC Size: 1     <<<<<<<<\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 159MB/s]  \n                                                                              \r","output_type":"stream"},{"name":"stdout","text":"[[2668, 832], [0, 0]]\nEpoch 1/40\t Train Loss: 0.5738 - Val Loss: 0.5567 - Error Rate: 0.2377 - F1 Score: 0.8651 - (FPR: 0.2377 - FNR: 0.0000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"[[2668, 832], [0, 0]]\nEpoch 2/40\t Train Loss: 0.5676 - Val Loss: 0.5522 - Error Rate: 0.2377 - F1 Score: 0.8651 - (FPR: 0.2377 - FNR: 0.0000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                              \r","output_type":"stream"},{"name":"stdout","text":"[[2668, 832], [0, 0]]\nEpoch 3/40\t Train Loss: 0.5656 - Val Loss: 0.5503 - Error Rate: 0.2377 - F1 Score: 0.8651 - (FPR: 0.2377 - FNR: 0.0000)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m best_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m---> 28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrainingEpoch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m02d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     val_loss, errors, confusion_matrix \u001b[38;5;241m=\u001b[39m evaluate(val_dataloader, model, criterion, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidationEpoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, model, criterion, optimizer, checkpoint, desc, dir_checkpoint)\u001b[0m\n\u001b[1;32m      4\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39mdesc, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[1;32m      8\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n","Cell \u001b[0;32mIn[8], line 18\u001b[0m, in \u001b[0;36mCactusDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[0;32m---> 18\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[1;32m     20\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/functional.py:492\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[0;32m--> 492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mantialias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:472\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, antialias)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m interpolation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbicubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m out_dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m    470\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m--> 472\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43m_cast_squeeze_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_cast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_cast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_squeeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_squeeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py:539\u001b[0m, in \u001b[0;36m_cast_squeeze_out\u001b[0;34m(img, need_cast, need_squeeze, out_dtype)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_cast:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39muint8, torch\u001b[38;5;241m.\u001b[39mint8, torch\u001b[38;5;241m.\u001b[39mint16, torch\u001b[38;5;241m.\u001b[39mint32, torch\u001b[38;5;241m.\u001b[39mint64):\n\u001b[1;32m    538\u001b[0m         \u001b[38;5;66;03m# it is better to round before cast\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(out_dtype)\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}