{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Challenge 1"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running locally\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","def is_running_on_kaggle():\n","    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\"\n","DATA_PATH = '/kaggle/input/aerial-cactus/' if is_running_on_kaggle() else 'data/'\n","print('Running on Kaggle' if is_running_on_kaggle() else 'Running locally')\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["Import required libraries"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.data import Dataset\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available. Using GPU.\n"]}],"source":["# Check if CUDA is available\n","if torch.cuda.is_available():\n","    # Set device to CUDA\n","    device = torch.device(\"cuda\")\n","    print(\"CUDA is available. Using GPU.\")\n","else:\n","    # Set device to CPU\n","    device = torch.device(\"cpu\")\n","    print(\"CUDA is not available. Using CPU.\")"]},{"cell_type":"markdown","metadata":{},"source":["Define some useful constants."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["ANNOTATIONS_FILE = DATA_PATH + 'train.csv'\n","IMG_DIR = DATA_PATH + 'train/train/'\n","CHECKPOINT_DIR = '/kaggle/working/' if is_running_on_kaggle() else 'checkpoints/'\n","\n","SEED = 42\n","BATCH_SIZE = 64\n","LEARNING_RATE = 1e-3\n","TRAIN_SPLIT = 0.8\n","EPOCHS = 5"]},{"cell_type":"markdown","metadata":{},"source":["Set the manual seed."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7d185829b090>"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["Extend **Dataset** class for the **DatasetLoader** (define a mapping for images and labels)."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from torchvision.io import read_image\n","\n","class CactusDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"markdown","metadata":{},"source":["TODO: Try to preprocess like in ImProc"]},{"cell_type":"markdown","metadata":{},"source":["Instanciate a **Dataset** object on the training (+validation) data."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Imagenet mean and std\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import torchvision.transforms as transforms\n","\n","# Transformation for the image data\n","transform = transforms.Compose([\n","    transforms.Resize(256, interpolation=transforms.InterpolationMode.BILINEAR),\n","    transforms.CenterCrop(224),\n","    transforms.ConvertImageDtype(torch.float32),\n","    transforms.Normalize(mean=mean, std=std),\n","])"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Create the dataset object\n","trainval_data = CactusDataset(ANNOTATIONS_FILE, IMG_DIR, transform=transform)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 224, 224])\n"]}],"source":["# Print the shape of the first image in the dataset\n","print(trainval_data[0][0].shape)"]},{"cell_type":"markdown","metadata":{},"source":["Split the dataset into train + validation"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 14000\n","Validation set size: 3500\n"]}],"source":["from torch.utils.data import random_split\n","\n","# Define the sizes of training and validation sets\n","train_size = int(TRAIN_SPLIT * len(trainval_data))\n","val_size = len(trainval_data) - train_size\n","\n","# Split the dataset into training and validation sets\n","train_data, val_data = random_split(trainval_data, [train_size, val_size])\n","\n","# Print the sizes of the training and validation sets\n","print(\"Training set size:\", len(train_data))\n","print(\"Validation set size:\", len(val_data))"]},{"cell_type":"markdown","metadata":{},"source":["Let's define our first model.\\\n","We are going to use the ResNet18 pretrained model and then we are going to add 1 linear FC output layer. The output will be a real value that we will feed into a Sigmoid function to squash it into the $[0, 1]$ interval, and we will do the classification by comparing the output of the Sigmoid with the $0.5$ treshold.\\\n","Since we don't want to adjust ResNet18 weights, we are going to set the **requires_grad** property to **False** for each of its parameters."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from torchvision.models import resnet18, ResNet18_Weights\n","\n","class ResnetClassificator(nn.Module):\n","    def __init__(self):\n","        super(ResnetClassificator, self).__init__()\n","        resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","        for param in resnet.parameters():\n","            param.requires_grad = False\n","        \n","        self.resnet = resnet\n","        last_layer_size = resnet.fc.out_features # 1000\n","        self.fc = nn.Linear(last_layer_size, 1)\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.fc(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["At this point, let's train our model."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Create the model\n","model = ResnetClassificator().to(device)\n","\n","# Define the loss function and optimizer\n","criterion = nn.BCEWithLogitsLoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Create the dataloaders\n","train_dataloader = torch.utils.data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_data, BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Utility function for saving epochs checkpoints\n","def save_checkpoint(model, optimizer, loss, desc):\n","    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_{desc}.pt')\n","    torch.save({\n","        'desc': desc,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': loss,\n","    }, checkpoint_path)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def train_model(train_dataloader, model, criterion, optimizer, checkpoint=False, desc='Training'):\n","    avg_train_loss = 0\n","    train_bar = tqdm(train_dataloader, desc=desc, leave=True)\n","    \n","    model.train()\n","    for X, y in train_bar:\n","        X = X.to(device)\n","        y = y.to(device)\n","\n","        # Compute prediction and loss\n","        pred = model(X.float())\n","        y = y.reshape(-1, 1).float()\n","        loss = criterion(pred, y)\n","\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the average loss\n","        avg_train_loss += loss.item() * len(y)\n","\n","        # Update the loading bar\n","        train_bar.set_postfix({'loss': loss.item()})\n","        \n","    avg_train_loss = avg_train_loss / len(train_dataloader.dataset)\n","    train_bar.set_postfix({'loss': avg_train_loss})\n","    train_bar.close()\n","\n","    if checkpoint:\n","        save_checkpoint(model, optimizer, avg_train_loss, desc)\n","\n","    return avg_train_loss"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def evaluate(val_dataloader, model, desc='Validation'):\n","    avg_val_loss = 0\n","    errors = []\n","    val_bar = tqdm(val_dataloader, desc=desc, leave=False)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for X, y in val_bar:\n","            X = X.to(device)\n","            y = y.to(device)\n","\n","            # Compute prediction and loss\n","            pred = model(X.float())\n","            y = y.reshape(-1, 1).float()\n","            loss = criterion(pred, y)\n","\n","            # Save errors for error rate\n","            pred = nn.Sigmoid()(pred) > 0.5\n","            errors += pred != y\n","\n","            # Update the average loss\n","            avg_val_loss += loss.item() * len(y)\n","\n","            # Update the loading bar\n","            val_bar.set_postfix({'loss': loss.item()})\n","    \n","    avg_val_loss = avg_val_loss / len(val_dataloader.dataset)\n","    val_bar.set_postfix({'loss': avg_val_loss})\n","    val_bar.close()\n","    return avg_val_loss, errors\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["TrainingEpoch01:   0%|          | 0/219 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["TrainingEpoch01:   3%|â–Ž         | 7/219 [00:03<01:37,  2.17it/s, loss=0.293]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m val_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTrainingEpoch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m02d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     val_loss, errors \u001b[38;5;241m=\u001b[39m evaluate(val_dataloader, model, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidationEpoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n","Cell \u001b[0;32mIn[16], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, model, criterion, optimizer, checkpoint, desc)\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Update the average loss\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m avg_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Update the loading bar\u001b[39;00m\n\u001b[1;32m     25\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem()})\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_losses = []\n","val_losses = []\n","for epoch in range(EPOCHS):\n","    train_loss = train_model(train_dataloader, model, criterion, optimizer, checkpoint=True, desc=f'TrainingEpoch{(epoch + 1):02d}')\n","    val_loss, errors = evaluate(val_dataloader, model, desc=f'ValidationEpoch{(epoch + 1):02d}')\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    error_rate = sum(errors) / len(val_data)\n","    print(f'Epoch {epoch + 1}/{EPOCHS} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f} - Error Rate: {error_rate.item():.4f}')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4736944,"sourceId":8035608,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
