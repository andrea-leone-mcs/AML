{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Challenge 1"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Running locally\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","import os\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","def is_running_on_kaggle():\n","    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ and os.environ[\"KAGGLE_KERNEL_RUN_TYPE\"] == \"Interactive\"\n","data_path = '/kaggle/input/aerial-cactus/' if is_running_on_kaggle() else 'data/'\n","print('Running on Kaggle' if is_running_on_kaggle() else 'Running locally')\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CUDA is available. Using GPU.\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x7038e60e7150>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","# Check if CUDA is available\n","if torch.cuda.is_available():\n","    # Set device to CUDA\n","    device = torch.device(\"cuda\")\n","    print(\"CUDA is available. Using GPU.\")\n","else:\n","    # Set device to CPU\n","    device = torch.device(\"cpu\")\n","    print(\"CUDA is not available. Using CPU.\")\n","\n","torch.manual_seed(42)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import torch\n","import torchvision\n","from torch.utils.data import Dataset\n","from torchvision.transforms import ToTensor\n","\n","# Imports\n","import warnings\n","import matplotlib.pyplot as plt\n","from torchvision.io import read_image\n","\n","class CactusDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = read_image(img_path)\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["annotations_file = data_path + 'train.csv'\n","img_dir = data_path + 'train/train/'\n","\n","BATCH_SIZE = 64\n","LEARNING_RATE = 1e-3"]},{"cell_type":"markdown","metadata":{},"source":["TODO: Try to preprocess like in ImProc"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 224, 224])\n"]}],"source":["from torchvision.models import resnet18\n","from torchvision.transforms import Normalize\n","import torchvision.transforms as transforms\n","\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","transform = transforms.Compose([\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ConvertImageDtype(torch.float32),\n","    transforms.Normalize(mean=mean, std=std),\n","])\n","training_dataset = CactusDataset(annotations_file, img_dir, transform=transform)\n","\n","print(training_dataset[0][0].shape)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 10500\n","Validation set size: 7000\n"]}],"source":["from torch.utils.data import random_split\n","\n","# Define the sizes of training and validation sets\n","train_size = int(0.6 * len(training_dataset))\n","val_size = len(training_dataset) - train_size\n","\n","# Split the dataset into training and validation sets\n","train_data, val_data = random_split(training_dataset, [train_size, val_size])\n","\n","# Print the sizes of the training and validation sets\n","print(\"Training set size:\", len(train_data))\n","print(\"Validation set size:\", len(val_data))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class ResnetClassificator(nn.Module):\n","    def __init__(self):\n","        super(ResnetClassificator, self).__init__()\n","        self.resnet = resnet18(pretrained=True)\n","        last_layer_size = self.resnet.fc.out_features # 1000        \n","        self.fc1 = nn.Linear(last_layer_size, 8)\n","        self.fc2 = nn.Linear(8, 1)\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/andrewl73/anaconda3/envs/AML/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/home/andrewl73/anaconda3/envs/AML/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["ResnetClassificator(\n","  (resnet): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=1000, bias=True)\n","  )\n","  (fc1): Linear(in_features=1000, out_features=8, bias=True)\n","  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",")"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["model = ResnetClassificator()\n","model.to(device)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["batch 0 | loss: 0.730981  [    0/10500]\n","batch 10 | loss: 0.131074  [  640/10500]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# for t in range(epochs):\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#     print(f\"Epoch {t+1}\\n-------------------------------\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#     train_model(train_dataloader, model, criterion, optimizer)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_dataloader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(train_dataloader, model, criterion, optimizer):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m---> 11\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","# Train the model\n","train_dataloader = torch.utils.data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n","\n","do_train = True\n","\n","def train_model(train_dataloader, model, criterion, optimizer):\n","    for batch, (X, y) in enumerate(train_dataloader):\n","        X = X.to(device)\n","        y = y.to(device)\n","        # Compute prediction and loss\n","        pred = model(X.float())\n","        # print(pred)\n","        # print('Interval: ', pred.min(), pred.max())\n","        y = y.reshape(-1, 1).float()\n","        loss = criterion(pred, y)\n","        # Backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if batch % 10 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"batch {batch} | loss: {loss:>7f}  [{current:>5d}/{train_size:>5d}]\")\n","\n","if do_train:\n","    epochs = 5\n","    # for t in range(epochs):\n","    #     print(f\"Epoch {t+1}\\n-------------------------------\")\n","    #     train_model(train_dataloader, model, criterion, optimizer)\n","    train_model(train_dataloader, model, criterion, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["loss: 0.135296  [    0/ 7000]\n","loss: 0.207270  [  640/ 7000]\n","loss: 0.045310  [ 1280/ 7000]\n","loss: 0.151156  [ 1920/ 7000]\n","loss: 0.201006  [ 2560/ 7000]\n","loss: 0.139554  [ 3200/ 7000]\n","loss: 0.243063  [ 3840/ 7000]\n","loss: 0.247136  [ 4480/ 7000]\n","loss: 0.278913  [ 5120/ 7000]\n","loss: 0.092146  [ 5760/ 7000]\n","loss: 0.104191  [ 6400/ 7000]\n"]}],"source":["val_dataloader = torch.utils.data.DataLoader(val_data, BATCH_SIZE)\n","model.eval()\n","\n","errors = []\n","\n","with torch.no_grad():\n","    for batch, (X, y) in enumerate(val_dataloader):\n","        X = X.to(device)\n","        y = y.to(device)\n","        pred = model(X.float())\n","        y = y.reshape(-1, 1).float()\n","        loss = criterion(pred, y)\n","\n","        pred = nn.Sigmoid()(pred) > 0.5\n","        errors += pred != y\n","        if batch % 10 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{val_size:>5d}]\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Error rate: 0.04542857142857143\n","Number of errors: 318 over 7000 samples\n"]}],"source":["# print(matches)\n","errors = torch.tensor(errors)\n","print(f'Error rate: {errors.sum().item() / val_size}')\n","print(f'Number of errors: {errors.sum().item()} over {len(errors)} samples')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4736944,"sourceId":8035608,"sourceType":"datasetVersion"}],"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
