{"cells":[{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Running on local machine\n","Using cuda device\n"]}],"source":["import pandas as pd\n","import torch\n","import os\n","\n","def is_running_on_kaggle():\n","    return 'KAGGLE_URL_BASE' in os.environ\n","\n","print('Running on Kaggle' if is_running_on_kaggle() else 'Running on local machine')\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using {device} device')"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["DATA_PATH = '/kaggle/input/sentiment-analysis-dataset/' if is_running_on_kaggle() else 'data/'\n","TRAIN_FILE = DATA_PATH + 'train.csv'\n","TEST_FILE = DATA_PATH + 'test.csv'\n","TRAIN_SPLIT = 0.8\n","BLOCK_SIZE = 32"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x778302970df0>"]},"execution_count":78,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(0)"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","for word in ['not', 'no', 'nor']:\n","    stop_words.remove(word)"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["from nltk.stem import PorterStemmer\n","import nltk\n","import preprocessor as p\n","import string\n","import re\n","import contractions\n","\n","def clean_tweet(row):\n","    text = row['text']\n","    p.set_options(p.OPT.URL, p.OPT.EMOJI)\n","    clean_text = p.clean(text)\n","    lower_text = clean_text.lower()\n","    \n","    expanded_text = contractions.fix(lower_text)\n","    fixed_text = expanded_text.replace('[^\\w\\s]',' ').replace('\\s\\s+', ' ').translate(str.maketrans('', '', string.punctuation))\n","    return fixed_text\n","\n","def tokenize_tweet(row):\n","    text = row['cleaned_text']\n","    tokens = p.tokenize(text).split()\n","    st = PorterStemmer()\n","    tokens = [st.stem(word) for word in tokens]\n","    filtered_tokens = [w for w in tokens if not w in stop_words]\n","    return filtered_tokens"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["# df.head(10)"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2024-05-31T14:14:34.034452Z","iopub.status.busy":"2024-05-31T14:14:34.033927Z","iopub.status.idle":"2024-05-31T14:14:56.909026Z","shell.execute_reply":"2024-05-31T14:14:56.907828Z","shell.execute_reply.started":"2024-05-31T14:14:34.034412Z"},"trusted":true},"outputs":[],"source":["# df['cleaned_text']= df.apply(clean_tweet, axis=1)\n","# df.head(10)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["# df['tokens'] = df.apply(tokenize_tweet, axis=1)\n","# df.head(10)"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","\n","df = pd.read_csv(TRAIN_FILE)\n","\n","# Define the sizes of training and validation sets\n","train_df, val_df = train_test_split(df, train_size=TRAIN_SPLIT, random_state=42)"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["import torch\n","import random\n","from torch.utils.data import Dataset\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, df, vocab=None):\n","        self.data = df\n","        self.data['cleaned_text'] = self.data.apply(clean_tweet, axis=1)\n","        self.data['tokens'] = self.data.apply(tokenize_tweet, axis=1)\n","        if 'sentiment' in self.data.columns:\n","            label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}\n","            df['label'] = df['sentiment'].map(label_mapping)\n","    \n","    def build_vocab(self):\n","        all_tokens = set([token for tokens in self.data['tokens'] for token in tokens])\n","        vocab = {token: i for i, token in enumerate(all_tokens)}\n","        vocab['<UNK>'] = len(vocab)\n","        self.vocab = vocab\n","\n","    def encode_sentence(self, tokens):\n","        encoded = torch.tensor([self.vocab[token] if token in self.vocab else self.vocab['<UNK>'] for token in tokens], dtype=torch.long)\n","        pos = [True] * len(encoded) + [False] * (BLOCK_SIZE-len(encoded))\n","        random.shuffle(pos)\n","        padded = torch.ones(BLOCK_SIZE, dtype=torch.long) * self.vocab['<UNK>']\n","        i = 0\n","        for j, p in enumerate(pos):\n","            if p:\n","                padded[j] = encoded[i]\n","                i += 1\n","        return padded\n","    \n","    def set_vocab(self, vocab):\n","        self.vocab = vocab\n","\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    def __getitem__(self, idx):\n","        if 'encoding' not in self.data.columns:\n","            self.data['encoding'] = self.data['tokens'].apply(self.encode_sentence)\n","        encoding = self.data.iloc[idx]['encoding']\n","        label = self.data.iloc[idx]['label'] if 'label' in self.data.columns else None\n","        return encoding, label"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["# Create an instance of the TrainingDataset\n","train_data = SentimentDataset(train_df)\n","val_data = SentimentDataset(val_df)\n","train_data.build_vocab()\n","val_data.set_vocab(train_data.vocab)"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: 19785\n","Validation set size: 4947\n"]}],"source":["# Print the sizes of the training and validation sets\n","print(\"Training set size:\", len(train_data))\n","print(\"Validation set size:\", len(val_data))"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"data":{"text/plain":["(8570     [tensor(18258), tensor(18258), tensor(18258), ...\n"," 24337    [tensor(18258), tensor(18258), tensor(18258), ...\n"," 21593    [tensor(1545), tensor(18258), tensor(18258), t...\n"," 19424    [tensor(4392), tensor(18258), tensor(18258), t...\n"," 11547    [tensor(18258), tensor(2669), tensor(18258), t...\n"," 22493    [tensor(18258), tensor(18258), tensor(18258), ...\n"," 20805    [tensor(18258), tensor(18258), tensor(18258), ...\n"," 9419     [tensor(18258), tensor(18258), tensor(18258), ...\n"," 14627    [tensor(18258), tensor(18258), tensor(18258), ...\n"," 1608     [tensor(18258), tensor(18258), tensor(18258), ...\n"," Name: encoding, dtype: object,\n"," 8570     1\n"," 24337    1\n"," 21593    0\n"," 19424    2\n"," 11547    1\n"," 22493    1\n"," 20805    2\n"," 9419     2\n"," 14627    0\n"," 1608     1\n"," Name: label, dtype: int64)"]},"metadata":{},"output_type":"display_data"}],"source":["display(train_data[:10])"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class TransformerSentimentModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_classes, num_heads, hidden_dim, num_layers, max_seq_len):\n","        super(TransformerSentimentModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_len, embed_size))\n","        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_size,\n","                                                   nhead=num_heads,\n","                                                   dim_feedforward=hidden_dim,\n","                                                   dropout=0.2)\n","        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","        self.fc = nn.Linear(embed_size, num_classes)\n","\n","    def forward(self, src, src_key_padding_mask=None):\n","        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n","        src = src.permute(1, 0, 2)  # (S, N, E) for transformer\n","        output = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n","        output = output.mean(dim=0)  # Global average pooling\n","        output = self.fc(output)\n","        return output"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/andrewl73/anaconda3/envs/AML/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100 => Train Loss: 1.1026, Validation Loss: 1.0890, F1: 0.1892\n","Accuracy: 0.3964\n"]},{"data":{"text/plain":["array([[   0, 1406,    0],\n","       [   0, 1961,    0],\n","       [   0, 1580,    0]])"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2/100 => Train Loss: 1.0885, Validation Loss: 1.0907, F1: 0.1892\n","Accuracy: 0.3964\n"]},{"data":{"text/plain":["array([[   0, 1406,    0],\n","       [   0, 1961,    0],\n","       [   0, 1580,    0]])"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3/100 => Train Loss: 1.0887, Validation Loss: 1.0915, F1: 0.1892\n","Accuracy: 0.3964\n"]},{"data":{"text/plain":["array([[   0, 1406,    0],\n","       [   0, 1961,    0],\n","       [   0, 1580,    0]])"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[102], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 32\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader)\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.metrics import f1_score, confusion_matrix\n","\n","torch.manual_seed(0)\n","\n","# model = SentimentClassifier(len(dataset.vocab), 128)\n","model = TransformerSentimentModel(vocab_size=len(train_data.vocab),\n","                                  embed_size=48,\n","                                  num_classes=3,\n","                                  num_heads=8,\n","                                  hidden_dim=2048,\n","                                  num_layers=6,\n","                                  max_seq_len=BLOCK_SIZE)\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n","\n","train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n","val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=128, shuffle=True)\n","\n","epochs = 100\n","for epoch in range(epochs):\n","    model.train()\n","    train_loss = 0.0\n","    for inputs, labels in train_dataloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","    train_loss /= len(train_dataloader)\n","    \n","    model.eval()\n","    val_loss = 0.0\n","    all_preds = []\n","    all_labels = []\n","    with torch.no_grad():\n","        for inputs, labels in val_dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","            \n","            preds = torch.argmax(outputs, dim=1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","    val_loss /= len(val_dataloader)\n","    \n","    f1 = f1_score(all_labels, all_preds, average='macro')\n","    cm = confusion_matrix(all_labels, all_preds)\n","    acc = sum([1 if p == l else 0 for p, l in zip(all_preds, all_labels)]) / len(all_preds)\n","    \n","    print(f\"Epoch {epoch+1}/{epochs} => Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, F1: {f1:.4f}\")\n","    print(f\"Accuracy: {acc:.4f}\")\n","    display(cm)"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["18259\n","18259\n","0\n"]},{"data":{"text/plain":["set()"]},"metadata":{},"output_type":"display_data"}],"source":["train_vocab = set(train_data.vocab)\n","val_vocab = set(val_data.vocab)\n","missing_vocab = val_vocab - train_vocab\n","print(len(train_vocab))\n","print(len(val_vocab))\n","print(len(missing_vocab))\n","display(missing_vocab)"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5206\n"]}],"source":["df_test = pd.read_csv(TEST_FILE)\n","dataset_test = SentimentDataset(df_test)\n","dataset_test.build_vocab()\n","print(len(dataset_test.vocab))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4765148,"sourceId":8074783,"sourceType":"datasetVersion"}],"dockerImageVersionId":30715,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
